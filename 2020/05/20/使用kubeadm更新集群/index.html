<!doctype html><html lang=zh-cn dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>使用Kubeadm更新集群 | 戴先森的学习笔记</title>
<meta name=keywords content="kubeadm,kubernetes"><meta name=description content="前面我们已经通过 kubeadm 安装了一套集群
当时安装的版本为 v1.16.3，现在版本已经更新到 1.18.x 了，所以我们需要对它进行升级

k8s 不支持跨多个大版本升级，所以我们如果想要升级到 1.18.x，就得先升级到 1.17.x 版本"><meta name=author content="Xijun Dai"><link rel=canonical href=https://linuxyunwei.com/2020/05/20/%E4%BD%BF%E7%94%A8kubeadm%E6%9B%B4%E6%96%B0%E9%9B%86%E7%BE%A4/><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://linuxyunwei.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://linuxyunwei.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://linuxyunwei.com/favicon-32x32.png><link rel=apple-touch-icon href=https://linuxyunwei.com/apple-touch-icon.png><link rel=mask-icon href=https://linuxyunwei.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh-cn href=https://linuxyunwei.com/2020/05/20/%E4%BD%BF%E7%94%A8kubeadm%E6%9B%B4%E6%96%B0%E9%9B%86%E7%BE%A4/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://linuxyunwei.com/2020/05/20/%E4%BD%BF%E7%94%A8kubeadm%E6%9B%B4%E6%96%B0%E9%9B%86%E7%BE%A4/"><meta property="og:site_name" content="戴先森的学习笔记"><meta property="og:title" content="使用Kubeadm更新集群"><meta property="og:description" content="前面我们已经通过 kubeadm 安装了一套集群
当时安装的版本为 v1.16.3，现在版本已经更新到 1.18.x 了，所以我们需要对它进行升级
k8s 不支持跨多个大版本升级，所以我们如果想要升级到 1.18.x，就得先升级到 1.17.x 版本"><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-05-20T19:17:26+08:00"><meta property="article:modified_time" content="2020-05-20T19:17:26+08:00"><meta property="article:tag" content="Kubeadm"><meta property="article:tag" content="Kubernetes"><meta name=twitter:card content="summary"><meta name=twitter:title content="使用Kubeadm更新集群"><meta name=twitter:description content="前面我们已经通过 kubeadm 安装了一套集群
当时安装的版本为 v1.16.3，现在版本已经更新到 1.18.x 了，所以我们需要对它进行升级

k8s 不支持跨多个大版本升级，所以我们如果想要升级到 1.18.x，就得先升级到 1.17.x 版本"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"使用Kubeadm更新集群","item":"https://linuxyunwei.com/2020/05/20/%E4%BD%BF%E7%94%A8kubeadm%E6%9B%B4%E6%96%B0%E9%9B%86%E7%BE%A4/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"使用Kubeadm更新集群","name":"使用Kubeadm更新集群","description":"前面我们已经通过 kubeadm 安装了一套集群\n当时安装的版本为 v1.16.3，现在版本已经更新到 1.18.x 了，所以我们需要对它进行升级\nk8s 不支持跨多个大版本升级，所以我们如果想要升级到 1.18.x，就得先升级到 1.17.x 版本\n","keywords":["kubeadm","kubernetes"],"articleBody":"前面我们已经通过 kubeadm 安装了一套集群\n当时安装的版本为 v1.16.3，现在版本已经更新到 1.18.x 了，所以我们需要对它进行升级\nk8s 不支持跨多个大版本升级，所以我们如果想要升级到 1.18.x，就得先升级到 1.17.x 版本\n升级步骤参考官方文档 Upgrading kubeadm clusters\n查看可升级的版本 如下可以看到，当前 1.17.5 为最新可升级版本\n$ yum list --showduplicates kubeadm --disableexcludes=kubernetes | grep 1.17 kubeadm.x86_64 1.17.0-0 kubernetes kubeadm.x86_64 1.17.1-0 kubernetes kubeadm.x86_64 1.17.2-0 kubernetes kubeadm.x86_64 1.17.3-0 kubernetes kubeadm.x86_64 1.17.4-0 kubernetes kubeadm.x86_64 1.17.5-0 kubernetes 升级 master 安装 1.17.5 版本 kubeadm $ yum install -y kubeadm-1.17.5-0 --disableexcludes=kubernetes $ kubeadm version kubeadm version: \u0026version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.5\", GitCommit:\"e0fccafd69541e3750d460ba0f9743b90336f24f\", GitTreeState:\"clean\", BuildDate:\"2020-04-16T11:41:38Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"} 给 master 节点打上污点并驱逐所有 pod $ kubectl drain k8s-master --ignore-daemonsets --delete-local-data node/k8s-master already cordoned node/k8s-master evicted 升级前的检查 $ kubeadm upgrade plan [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [preflight] Running pre-flight checks. [preflight] Some fatal errors occurred: [ERROR CoreDNSUnsupportedPlugins]: there are unsupported plugins in the CoreDNS Corefile [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...` To see the stack trace of this error execute with --v=5 or higher 如上出现警告信息 [ERROR CoreDNSUnsupportedPlugins]: there are unsupported plugins in the CoreDNS Corefile\n通过如下命令查看 CoreDNS 的配置文件，如果确认没有问题可以通过参数 --ignore-preflight-errors=CoreDNSUnsupportedPlugins忽略，github 上也有相关issue\nkubectl get configmap -n kube-system coredns -oyaml 此处选择忽略再次重试\n$ kubeadm upgrade plan --ignore-preflight-errors=CoreDNSUnsupportedPlugins [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [preflight] Running pre-flight checks. [WARNING CoreDNSUnsupportedPlugins]: there are unsupported plugins in the CoreDNS Corefile [upgrade] Making sure the cluster is healthy: [upgrade] Fetching available versions to upgrade to [upgrade/versions] Cluster version: v1.16.3 [upgrade/versions] kubeadm version: v1.17.5 I0520 19:40:07.623240 6896 version.go:251] remote version is much newer: v1.18.2; falling back to: stable-1.17 [upgrade/versions] Latest stable version: v1.17.5 [upgrade/versions] Latest version in the v1.16 series: v1.16.9 Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply': COMPONENT CURRENT AVAILABLE Kubelet 6 x v1.16.3 v1.16.9 Upgrade to the latest version in the v1.16 series: COMPONENT CURRENT AVAILABLE API Server v1.16.3 v1.16.9 Controller Manager v1.16.3 v1.16.9 Scheduler v1.16.3 v1.16.9 Kube Proxy v1.16.3 v1.16.9 CoreDNS 1.6.2 1.6.5 Etcd 3.3.15 3.3.17-0 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.16.9 _____________________________________________________________________ Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply': COMPONENT CURRENT AVAILABLE Kubelet 6 x v1.16.3 v1.17.5 Upgrade to the latest stable version: COMPONENT CURRENT AVAILABLE API Server v1.16.3 v1.17.5 Controller Manager v1.16.3 v1.17.5 Scheduler v1.16.3 v1.17.5 Kube Proxy v1.16.3 v1.17.5 CoreDNS 1.6.2 1.6.5 Etcd 3.3.15 3.4.3-0 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.17.5 _____________________________________________________________________ 执行升级 $ kubeadm upgrade apply v1.17.5 --ignore-preflight-errors=CoreDNSUnsupportedPlugins [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [preflight] Running pre-flight checks. [WARNING CoreDNSUnsupportedPlugins]: there are unsupported plugins in the CoreDNS Corefile [upgrade] Making sure the cluster is healthy: [upgrade/version] You have chosen to change the cluster version to \"v1.17.5\" [upgrade/versions] Cluster version: v1.16.3 [upgrade/versions] kubeadm version: v1.17.5 [upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y [upgrade/prepull] Will prepull images for components [kube-apiserver kube-controller-manager kube-scheduler etcd] [upgrade/prepull] Prepulling image for component kube-apiserver. [upgrade/prepull] Prepulling image for component kube-controller-manager. [upgrade/prepull] Prepulling image for component etcd. [upgrade/prepull] Prepulling image for component kube-scheduler. [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-etcd [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-etcd [upgrade/prepull] Prepulled image for component kube-apiserver. [upgrade/prepull] Prepulled image for component kube-controller-manager. [upgrade/prepull] Prepulled image for component etcd. [upgrade/prepull] Prepulled image for component kube-scheduler. [upgrade/prepull] Successfully prepulled the images for all the control plane components [upgrade/apply] Upgrading your Static Pod-hosted control plane to version \"v1.17.5\"... Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965 Static pod: kube-controller-manager-k8s-master hash: 83e58d3548072b341a6faa0bbf8427dc Static pod: kube-scheduler-k8s-master hash: e8486b59c2c8408b07026a560746b02c [upgrade/etcd] Upgrading to TLS for etcd Static pod: etcd-k8s-master hash: 7d86fcdbd639f4fd16605f99fbffa6e4 [upgrade/staticpods] Preparing for \"etcd\" upgrade [upgrade/staticpods] Renewing etcd-server certificate [upgrade/staticpods] Renewing etcd-peer certificate [upgrade/staticpods] Renewing etcd-healthcheck-client certificate [upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/etcd.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-05-20-19-53-29/etcd.yaml\" [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s) Static pod: etcd-k8s-master hash: 7d86fcdbd639f4fd16605f99fbffa6e4 Static pod: etcd-k8s-master hash: 0f55aa1d6eb94a8c4eca4c5f027fb643 [apiclient] Found 1 Pods for label selector component=etcd [upgrade/staticpods] Component \"etcd\" upgraded successfully! [upgrade/etcd] Waiting for etcd to become available [upgrade/staticpods] Writing new Static Pod manifests to \"/etc/kubernetes/tmp/kubeadm-upgraded-manifests198557268\" W0520 19:54:11.882414 28625 manifests.go:214] the default kube-apiserver authorization-mode is \"Node,RBAC\"; using \"Node,RBAC\" [upgrade/staticpods] Preparing for \"kube-apiserver\" upgrade [upgrade/staticpods] Renewing apiserver certificate [upgrade/staticpods] Renewing apiserver-kubelet-client certificate [upgrade/staticpods] Renewing front-proxy-client certificate [upgrade/staticpods] Renewing apiserver-etcd-client certificate [upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-apiserver.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-05-20-19-53-29/kube-apiserver.yaml\" [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s) Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965 Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965 Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965 Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965 Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965 Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965 Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965 Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965 Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965 Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965 Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965 Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965 Static pod: kube-apiserver-k8s-master hash: cb0329ceca81a08d2613d2eafe96927f [apiclient] Found 1 Pods for label selector component=kube-apiserver [upgrade/staticpods] Component \"kube-apiserver\" upgraded successfully! [upgrade/staticpods] Preparing for \"kube-controller-manager\" upgrade [upgrade/staticpods] Renewing controller-manager.conf certificate [upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-05-20-19-53-29/kube-controller-manager.yaml\" [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s) Static pod: kube-controller-manager-k8s-master hash: 83e58d3548072b341a6faa0bbf8427dc Static pod: kube-controller-manager-k8s-master hash: 57f625031b63d8f30d971821233b1d59 [apiclient] Found 1 Pods for label selector component=kube-controller-manager [upgrade/staticpods] Component \"kube-controller-manager\" upgraded successfully! [upgrade/staticpods] Preparing for \"kube-scheduler\" upgrade [upgrade/staticpods] Renewing scheduler.conf certificate [upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-scheduler.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-05-20-19-53-29/kube-scheduler.yaml\" [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s) Static pod: kube-scheduler-k8s-master hash: e8486b59c2c8408b07026a560746b02c Static pod: kube-scheduler-k8s-master hash: 9d8e79ec2b72cfad2458fb03205bc653 [apiclient] Found 1 Pods for label selector component=kube-scheduler [upgrade/staticpods] Component \"kube-scheduler\" upgraded successfully! [upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.17\" in namespace kube-system with the configuration for the kubelets in the cluster [kubelet-start] Downloading configuration for the kubelet from the \"kubelet-config-1.17\" ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [addons]: Migrating CoreDNS Corefile [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy [upgrade/successful] SUCCESS! Your cluster was upgraded to \"v1.17.5\". Enjoy! [upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so. 解除 master 节点污点 $ kubectl uncordon k8s-master node/k8s-master uncordoned 升级其他 master 节点 如果是高可用版本还需要升级其他的 master 节点\n本次集群只有单个 master,以下操作纯做记录\nyum install -y kubeadm-1.17.5-0 --disableexcludes=kubernetes kubeadm upgrade node kubeadm upgrade apply 升级所有 master 节点的 kubectl 与 kubelet yum install -y kubectl-1.17.5-0 kubelet-1.17.5-0 --disableexcludes=kubernetes 重启 master 节点 kubelet 服务 $ systemctl daemon-reload $ systemctl restart kubelet $ systemctl status kubelet ● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /usr/lib/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: active (running) since Wed 2020-05-20 20:11:47 CST; 12s ago Docs: https://kubernetes.io/docs/ Main PID: 2904 (kubelet) Tasks: 15 Memory: 25.9M CGroup: /system.slice/kubelet.service └─2904 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --cgroup-driver=systemd --network-plugin=cni --pod-infra... May 20 20:11:47 k8s-master kubelet[2904]: I0520 20:11:47.297946 2904 docker_service.go:260] Docker Info: \u0026{ID:BI53:OQC7:QV6C:SP3N:FBTT:N5ST:MK3I:CGLJ:X2JX:CJOH:CBEE:4GG2 Containers:30 ContainersRunning:22 Containe...rts d_type true] May 20 20:11:47 k8s-master kubelet[2904]: I0520 20:11:47.298023 2904 docker_service.go:273] Setting cgroupDriver to systemd May 20 20:11:47 k8s-master kubelet[2904]: I0520 20:11:47.305372 2904 remote_runtime.go:59] parsed scheme: \"\" May 20 20:11:47 k8s-master kubelet[2904]: I0520 20:11:47.305387 2904 remote_runtime.go:59] scheme \"\" not registered, fallback to default scheme May 20 20:11:47 k8s-master kubelet[2904]: I0520 20:11:47.305408 2904 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0 }] } May 20 20:11:47 k8s-master kubelet[2904]: I0520 20:11:47.305415 2904 clientconn.go:577] ClientConn switching balancer to \"pick_first\" May 20 20:11:47 k8s-master kubelet[2904]: I0520 20:11:47.305433 2904 remote_image.go:50] parsed scheme: \"\" May 20 20:11:47 k8s-master kubelet[2904]: I0520 20:11:47.305438 2904 remote_image.go:50] scheme \"\" not registered, fallback to default scheme May 20 20:11:47 k8s-master kubelet[2904]: I0520 20:11:47.305446 2904 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0 }] } May 20 20:11:47 k8s-master kubelet[2904]: I0520 20:11:47.305450 2904 clientconn.go:577] ClientConn switching balancer to \"pick_first\" Hint: Some lines were ellipsized, use -l to show in full. 确认 master 版本 $ kubectl get node k8s-master NAME STATUS ROLES AGE VERSION k8s-master Ready master 180d v1.17.5 升级 worker 节点 非特别说明，都是在 worker 节点上操作\n需要在所有 woker 节点上进行操作, 此处以 k8s-node01 节点为例\n更新 kubeadm yum install -y kubeadm-1.17.5-0 --disableexcludes=kubernetes 给 worker 节点打上污点并驱逐所有 pod 在 Master 节点上执行\n$ kubectl drain k8s-node01 --ignore-daemonsets --delete-local-data node/k8s-node01 cordoned evicting pod \"tekton-pipelines-webhook-694dc67fbb-kjbqv\" ... pod/tekton-pipelines-webhook-694dc67fbb-kjbqv evicted ... node/k8s-node01 evicted 升级 woker 节点 kubelet 配置 $ kubeadm upgrade node [upgrade] Reading configuration from the cluster... [upgrade] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [upgrade] Skipping phase. Not a control plane node. [kubelet-start] Downloading configuration for the kubelet from the \"kubelet-config-1.17\" ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [upgrade] The configuration for this node was successfully updated! [upgrade] Now you should go ahead and upgrade the kubelet package using your package manager. 升级 woker 节点 kubelet 与 kubectl 版本 yum install -y kubectl-1.17.5-0 kubelet-1.17.5-0 --disableexcludes=kubernetes 重启 kubelet 服务 $ systemctl daemon-reload $ systemctl restart kubelet $ systemctl status kubelet ● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /usr/lib/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: active (running) since Wed 2020-05-20 20:27:07 CST; 21s ago Docs: https://kubernetes.io/docs/ Main PID: 4202 (kubelet) Tasks: 18 Memory: 38.0M CGroup: /system.slice/kubelet.service └─4202 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/et... May 20 20:27:28 k8s-node01 kubelet[4202]: I0520 20:27:28.637150 4202 kuberuntime_manager.go:981] updating....0/24 May 20 20:27:28 k8s-node01 kubelet[4202]: I0520 20:27:28.637283 4202 docker_service.go:355] docker cri re...4,},} May 20 20:27:28 k8s-node01 kubelet[4202]: I0520 20:27:28.637397 4202 kubelet_network.go:77] Setting Pod C....0/24 May 20 20:27:28 k8s-node01 kubelet[4202]: I0520 20:27:28.638934 4202 kubelet_node_status.go:70] Attemptin...ode01 May 20 20:27:28 k8s-node01 kubelet[4202]: I0520 20:27:28.653131 4202 kubelet_node_status.go:112] Node k8s...tered May 20 20:27:28 k8s-node01 kubelet[4202]: I0520 20:27:28.653222 4202 kubelet_node_status.go:73] Successfu...ode01 May 20 20:27:28 k8s-node01 kubelet[4202]: E0520 20:27:28.657774 4202 kubelet.go:1844] skipping pod synchr...d yet May 20 20:27:28 k8s-node01 kubelet[4202]: I0520 20:27:28.659901 4202 setters.go:537] Node became not ready: {T... May 20 20:27:28 k8s-node01 kubelet[4202]: W0520 20:27:28.842260 4202 docker_sandbox.go:394] failed to read pod... May 20 20:27:28 k8s-node01 kubelet[4202]: E0520 20:27:28.857932 4202 kubelet.go:1844] skipping pod synchr...d yet Hint: Some lines were ellipsized, use -l to show in full. 解除 worker 节点污点 在 Master 节点上操作\n$ kubectl uncordon k8s-node01 node/k8s-node01 uncordoned 验证 worker 节点版本 $ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready master 180d v1.17.2 k8s-node01 Ready 180d v1.17.5 k8s-node02 Ready 180d v1.17.5 k8s-node03 Ready 180d v1.17.5 k8s-node04 Ready 180d v1.17.5 k8s-node05 Ready 180d v1.17.5 k8s-node06 Ready 180d v1.17.5 如上，再以相同的方法更新到 1.18.x 版本即可\n验证集群状态 检查系统组件 $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-7d4d547dd6-59xqn 1/1 Running 0 17m calico-node-8d8bm 1/1 Running 0 19h calico-node-hlv2h 1/1 Running 0 19h calico-node-nbt4b 1/1 Running 0 19h calico-node-p4s45 1/1 Running 4 19h calico-node-srb88 1/1 Running 0 19h calico-node-vfpfv 1/1 Running 0 19h coredns-546565776c-54gxr 1/1 Running 0 12m coredns-546565776c-zqhcb 1/1 Running 0 20m etcd-k8s-master 1/1 Running 0 20m istio-cni-node-42ngc 2/2 Running 0 10h istio-cni-node-6qngn 2/2 Running 0 10h istio-cni-node-8pzlj 2/2 Running 0 10h istio-cni-node-9p47g 2/2 Running 0 10h istio-cni-node-hqp75 2/2 Running 0 10h istio-cni-node-vmnbp 2/2 Running 10 10h kube-apiserver-k8s-master 1/1 Running 0 20m kube-controller-manager-k8s-master 1/1 Running 0 20m kube-proxy-86845 1/1 Running 0 19m kube-proxy-dnccs 1/1 Running 0 20m kube-proxy-grn6t 1/1 Running 0 19m kube-proxy-ltwdr 1/1 Running 0 19m kube-proxy-p5mn8 1/1 Running 0 19m kube-proxy-qrnb7 1/1 Running 0 19m kube-scheduler-k8s-master 1/1 Running 0 20m metrics-server-747dfbd64b-5nnlz 1/1 Running 0 23s 测试 DNS 功能 $ dig www.baidu.com @10.96.0.10 +short www.a.shifen.com. 112.80.248.75 112.80.248.76 $ dig kubernetes.default.svc.cluster.local @10.96.0.10 +short 10.96.0.1 $ kubectl run -n kube-ops -i --rm --restart=Never dummy --image=busybox:1.28 -- nslookup kubernetes.default Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes.default Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local pod \"dummy\" deleted 测试访问集群中的服务 $ kubectl get service -n default NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE whoami ClusterIP 10.96.179.57 80/TCP 21d $ curl http://10.96.179.57 Hostname: whoami-5b4bb9c787-xz87j IP: 127.0.0.1 IP: 172.16.217.76 RemoteAddr: 127.0.0.1:33338 GET / HTTP/1.1 Host: 10.96.179.57 User-Agent: curl/7.29.0 Accept: */* $ kubectl run curl -n kube-ops -it --image curlimages/curl --restart Never --rm -- curl whoami.default Hostname: whoami-5b4bb9c787-xz87j IP: 127.0.0.1 IP: 172.16.217.76 RemoteAddr: 127.0.0.1:35506 GET / HTTP/1.1 Host: whoami.default User-Agent: curl/7.70.0-DEV Accept: */* pod \"curl\" deleted ","wordCount":"2588","inLanguage":"zh-cn","datePublished":"2020-05-20T19:17:26+08:00","dateModified":"2020-05-20T19:17:26+08:00","author":{"@type":"Person","name":"Xijun Dai"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://linuxyunwei.com/2020/05/20/%E4%BD%BF%E7%94%A8kubeadm%E6%9B%B4%E6%96%B0%E9%9B%86%E7%BE%A4/"},"publisher":{"@type":"Organization","name":"戴先森的学习笔记","logo":{"@type":"ImageObject","url":"https://linuxyunwei.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://linuxyunwei.com/ accesskey=h title="戴先森的学习笔记 (Alt + H)">戴先森的学习笔记</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://linuxyunwei.com/ title=Home><span>Home</span></a></li><li><a href=https://linuxyunwei.com/articles/ title=Articles><span>Articles</span></a></li><li><a href=https://linuxyunwei.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://linuxyunwei.com/categories/ title=Categories><span>Categories</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://linuxyunwei.com/>Home</a></div><h1 class="post-title entry-hint-parent">使用Kubeadm更新集群</h1><div class=post-meta><span title='2020-05-20 19:17:26 +0800 +0800'>May 20, 2020</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;2588 words&nbsp;·&nbsp;Xijun Dai&nbsp;|&nbsp;<a href=https://github.com/linuxyunwei/linuxyunwei.com/tree/master/content/posts/kubeadm%e6%9b%b4%e6%96%b0%e9%9b%86%e7%be%a4.md rel="noopener noreferrer" target=_blank>修正错误</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#查看可升级的版本>查看可升级的版本</a></li><li><a href=#升级-master>升级 master</a><ul><li><a href=#安装-1175-版本-kubeadm>安装 1.17.5 版本 kubeadm</a></li><li><a href=#给-master-节点打上污点并驱逐所有-pod>给 master 节点打上污点并驱逐所有 pod</a></li><li><a href=#升级前的检查>升级前的检查</a></li><li><a href=#执行升级>执行升级</a></li><li><a href=#解除-master-节点污点>解除 master 节点污点</a></li><li><a href=#升级其他-master-节点>升级其他 master 节点</a></li><li><a href=#升级所有-master-节点的-kubectl-与-kubelet>升级所有 master 节点的 kubectl 与 kubelet</a></li><li><a href=#重启-master-节点-kubelet-服务>重启 master 节点 kubelet 服务</a></li><li><a href=#确认-master-版本>确认 master 版本</a></li></ul></li><li><a href=#升级-worker-节点>升级 worker 节点</a><ul><li><a href=#更新-kubeadm>更新 kubeadm</a></li><li><a href=#给-worker-节点打上污点并驱逐所有-pod>给 worker 节点打上污点并驱逐所有 pod</a></li><li><a href=#升级-woker-节点-kubelet-配置>升级 woker 节点 kubelet 配置</a></li><li><a href=#升级-woker-节点-kubelet-与-kubectl-版本>升级 woker 节点 kubelet 与 kubectl 版本</a></li><li><a href=#重启-kubelet-服务>重启 kubelet 服务</a></li><li><a href=#解除-worker-节点污点>解除 worker 节点污点</a></li><li><a href=#验证-worker-节点版本>验证 worker 节点版本</a></li></ul></li><li><a href=#验证集群状态>验证集群状态</a><ul><li><a href=#检查系统组件>检查系统组件</a></li><li><a href=#测试-dns-功能>测试 DNS 功能</a></li><li><a href=#测试访问集群中的服务>测试访问集群中的服务</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>前面我们已经通过 <a href=/2019/11/18/kubeadm-%E5%AE%89%E8%A3%85%E5%8D%95master%E9%9B%86%E7%BE%A4/>kubeadm 安装了一套集群</a></p><p>当时安装的版本为 v1.16.3，现在版本已经更新到 1.18.x 了，所以我们需要对它进行升级</p><blockquote><p>k8s 不支持跨多个大版本升级，所以我们如果想要升级到 1.18.x，就得先升级到 1.17.x 版本</p></blockquote><p>升级步骤参考官方文档 <a href=https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>Upgrading kubeadm clusters</a></p><h2 id=查看可升级的版本>查看可升级的版本<a hidden class=anchor aria-hidden=true href=#查看可升级的版本>#</a></h2><p>如下可以看到，当前 1.17.5 为最新可升级版本</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ yum list --showduplicates kubeadm --disableexcludes<span class=o>=</span>kubernetes <span class=p>|</span> grep 1.17
</span></span><span class=line><span class=cl>kubeadm.x86_64                       1.17.0-0                        kubernetes
</span></span><span class=line><span class=cl>kubeadm.x86_64                       1.17.1-0                        kubernetes
</span></span><span class=line><span class=cl>kubeadm.x86_64                       1.17.2-0                        kubernetes
</span></span><span class=line><span class=cl>kubeadm.x86_64                       1.17.3-0                        kubernetes
</span></span><span class=line><span class=cl>kubeadm.x86_64                       1.17.4-0                        kubernetes
</span></span><span class=line><span class=cl>kubeadm.x86_64                       1.17.5-0                        kubernetes
</span></span></code></pre></div><h2 id=升级-master>升级 master<a hidden class=anchor aria-hidden=true href=#升级-master>#</a></h2><h3 id=安装-1175-版本-kubeadm>安装 1.17.5 版本 kubeadm<a hidden class=anchor aria-hidden=true href=#安装-1175-版本-kubeadm>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ yum install -y kubeadm-1.17.5-0 --disableexcludes<span class=o>=</span>kubernetes
</span></span><span class=line><span class=cl>$ kubeadm version
</span></span><span class=line><span class=cl>kubeadm version: <span class=p>&amp;</span>version.Info<span class=o>{</span>Major:<span class=s2>&#34;1&#34;</span>, Minor:<span class=s2>&#34;17&#34;</span>, GitVersion:<span class=s2>&#34;v1.17.5&#34;</span>, GitCommit:<span class=s2>&#34;e0fccafd69541e3750d460ba0f9743b90336f24f&#34;</span>, GitTreeState:<span class=s2>&#34;clean&#34;</span>, BuildDate:<span class=s2>&#34;2020-04-16T11:41:38Z&#34;</span>, GoVersion:<span class=s2>&#34;go1.13.9&#34;</span>, Compiler:<span class=s2>&#34;gc&#34;</span>, Platform:<span class=s2>&#34;linux/amd64&#34;</span><span class=o>}</span>
</span></span></code></pre></div><h3 id=给-master-节点打上污点并驱逐所有-pod>给 master 节点打上污点并驱逐所有 pod<a hidden class=anchor aria-hidden=true href=#给-master-节点打上污点并驱逐所有-pod>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ kubectl drain k8s-master --ignore-daemonsets --delete-local-data
</span></span><span class=line><span class=cl>node/k8s-master already cordoned
</span></span><span class=line><span class=cl>node/k8s-master evicted
</span></span></code></pre></div><h3 id=升级前的检查>升级前的检查<a hidden class=anchor aria-hidden=true href=#升级前的检查>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ kubeadm upgrade plan
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/config<span class=o>]</span> Making sure the configuration is correct:
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/config<span class=o>]</span> Reading configuration from the cluster...
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/config<span class=o>]</span> FYI: You can look at this config file with <span class=s1>&#39;kubectl -n kube-system get cm kubeadm-config -oyaml&#39;</span>
</span></span><span class=line><span class=cl><span class=o>[</span>preflight<span class=o>]</span> Running pre-flight checks.
</span></span><span class=line><span class=cl><span class=o>[</span>preflight<span class=o>]</span> Some fatal errors occurred:
</span></span><span class=line><span class=cl> <span class=o>[</span>ERROR CoreDNSUnsupportedPlugins<span class=o>]</span>: there are unsupported plugins in the CoreDNS Corefile
</span></span><span class=line><span class=cl><span class=o>[</span>preflight<span class=o>]</span> If you know what you are doing, you can make a check non-fatal with <span class=sb>`</span>--ignore-preflight-errors<span class=o>=</span>...<span class=sb>`</span>
</span></span><span class=line><span class=cl>To see the stack trace of this error execute with --v<span class=o>=</span><span class=m>5</span> or higher
</span></span></code></pre></div><p>如上出现警告信息 <code>[ERROR CoreDNSUnsupportedPlugins]: there are unsupported plugins in the CoreDNS Corefile</code></p><p>通过如下命令查看 CoreDNS 的配置文件，如果确认没有问题可以通过参数 <code>--ignore-preflight-errors=CoreDNSUnsupportedPlugins</code>忽略，github 上也有相关<a href=https://github.com/kubernetes/kubernetes/issues/82889>issue</a></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>kubectl get configmap -n kube-system coredns -oyaml
</span></span></code></pre></div><p>此处选择忽略再次重试</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ kubeadm upgrade plan --ignore-preflight-errors<span class=o>=</span>CoreDNSUnsupportedPlugins
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/config<span class=o>]</span> Making sure the configuration is correct:
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/config<span class=o>]</span> Reading configuration from the cluster...
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/config<span class=o>]</span> FYI: You can look at this config file with <span class=s1>&#39;kubectl -n kube-system get cm kubeadm-config -oyaml&#39;</span>
</span></span><span class=line><span class=cl><span class=o>[</span>preflight<span class=o>]</span> Running pre-flight checks.
</span></span><span class=line><span class=cl> <span class=o>[</span>WARNING CoreDNSUnsupportedPlugins<span class=o>]</span>: there are unsupported plugins in the CoreDNS Corefile
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade<span class=o>]</span> Making sure the cluster is healthy:
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade<span class=o>]</span> Fetching available versions to upgrade to
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/versions<span class=o>]</span> Cluster version: v1.16.3
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/versions<span class=o>]</span> kubeadm version: v1.17.5
</span></span><span class=line><span class=cl>I0520 19:40:07.623240    <span class=m>6896</span> version.go:251<span class=o>]</span> remote version is much newer: v1.18.2<span class=p>;</span> falling back to: stable-1.17
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/versions<span class=o>]</span> Latest stable version: v1.17.5
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/versions<span class=o>]</span> Latest version in the v1.16 series: v1.16.9
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Components that must be upgraded manually after you have upgraded the control plane with <span class=s1>&#39;kubeadm upgrade apply&#39;</span>:
</span></span><span class=line><span class=cl>COMPONENT   CURRENT       AVAILABLE
</span></span><span class=line><span class=cl>Kubelet     <span class=m>6</span> x v1.16.3   v1.16.9
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Upgrade to the latest version in the v1.16 series:
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>COMPONENT            CURRENT   AVAILABLE
</span></span><span class=line><span class=cl>API Server           v1.16.3   v1.16.9
</span></span><span class=line><span class=cl>Controller Manager   v1.16.3   v1.16.9
</span></span><span class=line><span class=cl>Scheduler            v1.16.3   v1.16.9
</span></span><span class=line><span class=cl>Kube Proxy           v1.16.3   v1.16.9
</span></span><span class=line><span class=cl>CoreDNS              1.6.2     1.6.5
</span></span><span class=line><span class=cl>Etcd                 3.3.15    3.3.17-0
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>You can now apply the upgrade by executing the following command:
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> kubeadm upgrade apply v1.16.9
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>_____________________________________________________________________
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Components that must be upgraded manually after you have upgraded the control plane with <span class=s1>&#39;kubeadm upgrade apply&#39;</span>:
</span></span><span class=line><span class=cl>COMPONENT   CURRENT       AVAILABLE
</span></span><span class=line><span class=cl>Kubelet     <span class=m>6</span> x v1.16.3   v1.17.5
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Upgrade to the latest stable version:
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>COMPONENT            CURRENT   AVAILABLE
</span></span><span class=line><span class=cl>API Server           v1.16.3   v1.17.5
</span></span><span class=line><span class=cl>Controller Manager   v1.16.3   v1.17.5
</span></span><span class=line><span class=cl>Scheduler            v1.16.3   v1.17.5
</span></span><span class=line><span class=cl>Kube Proxy           v1.16.3   v1.17.5
</span></span><span class=line><span class=cl>CoreDNS              1.6.2     1.6.5
</span></span><span class=line><span class=cl>Etcd                 3.3.15    3.4.3-0
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>You can now apply the upgrade by executing the following command:
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> kubeadm upgrade apply v1.17.5
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>_____________________________________________________________________
</span></span></code></pre></div><h3 id=执行升级>执行升级<a hidden class=anchor aria-hidden=true href=#执行升级>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ kubeadm upgrade apply v1.17.5 --ignore-preflight-errors<span class=o>=</span>CoreDNSUnsupportedPlugins
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/config<span class=o>]</span> Making sure the configuration is correct:
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/config<span class=o>]</span> Reading configuration from the cluster...
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/config<span class=o>]</span> FYI: You can look at this config file with <span class=s1>&#39;kubectl -n kube-system get cm kubeadm-config -oyaml&#39;</span>
</span></span><span class=line><span class=cl><span class=o>[</span>preflight<span class=o>]</span> Running pre-flight checks.
</span></span><span class=line><span class=cl> <span class=o>[</span>WARNING CoreDNSUnsupportedPlugins<span class=o>]</span>: there are unsupported plugins in the CoreDNS Corefile
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade<span class=o>]</span> Making sure the cluster is healthy:
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/version<span class=o>]</span> You have chosen to change the cluster version to <span class=s2>&#34;v1.17.5&#34;</span>
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/versions<span class=o>]</span> Cluster version: v1.16.3
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/versions<span class=o>]</span> kubeadm version: v1.17.5
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/confirm<span class=o>]</span> Are you sure you want to proceed with the upgrade? <span class=o>[</span>y/N<span class=o>]</span>: y
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/prepull<span class=o>]</span> Will prepull images <span class=k>for</span> components <span class=o>[</span>kube-apiserver kube-controller-manager kube-scheduler etcd<span class=o>]</span>
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/prepull<span class=o>]</span> Prepulling image <span class=k>for</span> component kube-apiserver.
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/prepull<span class=o>]</span> Prepulling image <span class=k>for</span> component kube-controller-manager.
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/prepull<span class=o>]</span> Prepulling image <span class=k>for</span> component etcd.
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/prepull<span class=o>]</span> Prepulling image <span class=k>for</span> component kube-scheduler.
</span></span><span class=line><span class=cl><span class=o>[</span>apiclient<span class=o>]</span> Found <span class=m>1</span> Pods <span class=k>for</span> label selector k8s-app<span class=o>=</span>upgrade-prepull-kube-controller-manager
</span></span><span class=line><span class=cl><span class=o>[</span>apiclient<span class=o>]</span> Found <span class=m>0</span> Pods <span class=k>for</span> label selector k8s-app<span class=o>=</span>upgrade-prepull-kube-scheduler
</span></span><span class=line><span class=cl><span class=o>[</span>apiclient<span class=o>]</span> Found <span class=m>0</span> Pods <span class=k>for</span> label selector k8s-app<span class=o>=</span>upgrade-prepull-etcd
</span></span><span class=line><span class=cl><span class=o>[</span>apiclient<span class=o>]</span> Found <span class=m>1</span> Pods <span class=k>for</span> label selector k8s-app<span class=o>=</span>upgrade-prepull-kube-apiserver
</span></span><span class=line><span class=cl><span class=o>[</span>apiclient<span class=o>]</span> Found <span class=m>1</span> Pods <span class=k>for</span> label selector k8s-app<span class=o>=</span>upgrade-prepull-kube-scheduler
</span></span><span class=line><span class=cl><span class=o>[</span>apiclient<span class=o>]</span> Found <span class=m>1</span> Pods <span class=k>for</span> label selector k8s-app<span class=o>=</span>upgrade-prepull-etcd
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/prepull<span class=o>]</span> Prepulled image <span class=k>for</span> component kube-apiserver.
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/prepull<span class=o>]</span> Prepulled image <span class=k>for</span> component kube-controller-manager.
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/prepull<span class=o>]</span> Prepulled image <span class=k>for</span> component etcd.
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/prepull<span class=o>]</span> Prepulled image <span class=k>for</span> component kube-scheduler.
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/prepull<span class=o>]</span> Successfully prepulled the images <span class=k>for</span> all the control plane components
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/apply<span class=o>]</span> Upgrading your Static Pod-hosted control plane to version <span class=s2>&#34;v1.17.5&#34;</span>...
</span></span><span class=line><span class=cl>Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965
</span></span><span class=line><span class=cl>Static pod: kube-controller-manager-k8s-master hash: 83e58d3548072b341a6faa0bbf8427dc
</span></span><span class=line><span class=cl>Static pod: kube-scheduler-k8s-master hash: e8486b59c2c8408b07026a560746b02c
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/etcd<span class=o>]</span> Upgrading to TLS <span class=k>for</span> etcd
</span></span><span class=line><span class=cl>Static pod: etcd-k8s-master hash: 7d86fcdbd639f4fd16605f99fbffa6e4
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Preparing <span class=k>for</span> <span class=s2>&#34;etcd&#34;</span> upgrade
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Renewing etcd-server certificate
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Renewing etcd-peer certificate
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Renewing etcd-healthcheck-client certificate
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Moved new manifest to <span class=s2>&#34;/etc/kubernetes/manifests/etcd.yaml&#34;</span> and backed up old manifest to <span class=s2>&#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-05-20-19-53-29/etcd.yaml&#34;</span>
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Waiting <span class=k>for</span> the kubelet to restart the component
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> This might take a minute or longer depending on the component/version gap <span class=o>(</span>timeout 5m0s<span class=o>)</span>
</span></span><span class=line><span class=cl>Static pod: etcd-k8s-master hash: 7d86fcdbd639f4fd16605f99fbffa6e4
</span></span><span class=line><span class=cl>Static pod: etcd-k8s-master hash: 0f55aa1d6eb94a8c4eca4c5f027fb643
</span></span><span class=line><span class=cl><span class=o>[</span>apiclient<span class=o>]</span> Found <span class=m>1</span> Pods <span class=k>for</span> label selector <span class=nv>component</span><span class=o>=</span>etcd
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Component <span class=s2>&#34;etcd&#34;</span> upgraded successfully!
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/etcd<span class=o>]</span> Waiting <span class=k>for</span> etcd to become available
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Writing new Static Pod manifests to <span class=s2>&#34;/etc/kubernetes/tmp/kubeadm-upgraded-manifests198557268&#34;</span>
</span></span><span class=line><span class=cl>W0520 19:54:11.882414   <span class=m>28625</span> manifests.go:214<span class=o>]</span> the default kube-apiserver authorization-mode is <span class=s2>&#34;Node,RBAC&#34;</span><span class=p>;</span> using <span class=s2>&#34;Node,RBAC&#34;</span>
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Preparing <span class=k>for</span> <span class=s2>&#34;kube-apiserver&#34;</span> upgrade
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Renewing apiserver certificate
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Renewing apiserver-kubelet-client certificate
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Renewing front-proxy-client certificate
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Renewing apiserver-etcd-client certificate
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Moved new manifest to <span class=s2>&#34;/etc/kubernetes/manifests/kube-apiserver.yaml&#34;</span> and backed up old manifest to <span class=s2>&#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-05-20-19-53-29/kube-apiserver.yaml&#34;</span>
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Waiting <span class=k>for</span> the kubelet to restart the component
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> This might take a minute or longer depending on the component/version gap <span class=o>(</span>timeout 5m0s<span class=o>)</span>
</span></span><span class=line><span class=cl>Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965
</span></span><span class=line><span class=cl>Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965
</span></span><span class=line><span class=cl>Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965
</span></span><span class=line><span class=cl>Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965
</span></span><span class=line><span class=cl>Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965
</span></span><span class=line><span class=cl>Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965
</span></span><span class=line><span class=cl>Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965
</span></span><span class=line><span class=cl>Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965
</span></span><span class=line><span class=cl>Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965
</span></span><span class=line><span class=cl>Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965
</span></span><span class=line><span class=cl>Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965
</span></span><span class=line><span class=cl>Static pod: kube-apiserver-k8s-master hash: e541261a584becf7ccd0ebc447ad6965
</span></span><span class=line><span class=cl>Static pod: kube-apiserver-k8s-master hash: cb0329ceca81a08d2613d2eafe96927f
</span></span><span class=line><span class=cl><span class=o>[</span>apiclient<span class=o>]</span> Found <span class=m>1</span> Pods <span class=k>for</span> label selector <span class=nv>component</span><span class=o>=</span>kube-apiserver
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Component <span class=s2>&#34;kube-apiserver&#34;</span> upgraded successfully!
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Preparing <span class=k>for</span> <span class=s2>&#34;kube-controller-manager&#34;</span> upgrade
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Renewing controller-manager.conf certificate
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Moved new manifest to <span class=s2>&#34;/etc/kubernetes/manifests/kube-controller-manager.yaml&#34;</span> and backed up old manifest to <span class=s2>&#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-05-20-19-53-29/kube-controller-manager.yaml&#34;</span>
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Waiting <span class=k>for</span> the kubelet to restart the component
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> This might take a minute or longer depending on the component/version gap <span class=o>(</span>timeout 5m0s<span class=o>)</span>
</span></span><span class=line><span class=cl>Static pod: kube-controller-manager-k8s-master hash: 83e58d3548072b341a6faa0bbf8427dc
</span></span><span class=line><span class=cl>Static pod: kube-controller-manager-k8s-master hash: 57f625031b63d8f30d971821233b1d59
</span></span><span class=line><span class=cl><span class=o>[</span>apiclient<span class=o>]</span> Found <span class=m>1</span> Pods <span class=k>for</span> label selector <span class=nv>component</span><span class=o>=</span>kube-controller-manager
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Component <span class=s2>&#34;kube-controller-manager&#34;</span> upgraded successfully!
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Preparing <span class=k>for</span> <span class=s2>&#34;kube-scheduler&#34;</span> upgrade
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Renewing scheduler.conf certificate
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Moved new manifest to <span class=s2>&#34;/etc/kubernetes/manifests/kube-scheduler.yaml&#34;</span> and backed up old manifest to <span class=s2>&#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-05-20-19-53-29/kube-scheduler.yaml&#34;</span>
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Waiting <span class=k>for</span> the kubelet to restart the component
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> This might take a minute or longer depending on the component/version gap <span class=o>(</span>timeout 5m0s<span class=o>)</span>
</span></span><span class=line><span class=cl>Static pod: kube-scheduler-k8s-master hash: e8486b59c2c8408b07026a560746b02c
</span></span><span class=line><span class=cl>Static pod: kube-scheduler-k8s-master hash: 9d8e79ec2b72cfad2458fb03205bc653
</span></span><span class=line><span class=cl><span class=o>[</span>apiclient<span class=o>]</span> Found <span class=m>1</span> Pods <span class=k>for</span> label selector <span class=nv>component</span><span class=o>=</span>kube-scheduler
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/staticpods<span class=o>]</span> Component <span class=s2>&#34;kube-scheduler&#34;</span> upgraded successfully!
</span></span><span class=line><span class=cl><span class=o>[</span>upload-config<span class=o>]</span> Storing the configuration used in ConfigMap <span class=s2>&#34;kubeadm-config&#34;</span> in the <span class=s2>&#34;kube-system&#34;</span> Namespace
</span></span><span class=line><span class=cl><span class=o>[</span>kubelet<span class=o>]</span> Creating a ConfigMap <span class=s2>&#34;kubelet-config-1.17&#34;</span> in namespace kube-system with the configuration <span class=k>for</span> the kubelets in the cluster
</span></span><span class=line><span class=cl><span class=o>[</span>kubelet-start<span class=o>]</span> Downloading configuration <span class=k>for</span> the kubelet from the <span class=s2>&#34;kubelet-config-1.17&#34;</span> ConfigMap in the kube-system namespace
</span></span><span class=line><span class=cl><span class=o>[</span>kubelet-start<span class=o>]</span> Writing kubelet configuration to file <span class=s2>&#34;/var/lib/kubelet/config.yaml&#34;</span>
</span></span><span class=line><span class=cl><span class=o>[</span>bootstrap-token<span class=o>]</span> configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order <span class=k>for</span> nodes to get long term certificate credentials
</span></span><span class=line><span class=cl><span class=o>[</span>bootstrap-token<span class=o>]</span> configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
</span></span><span class=line><span class=cl><span class=o>[</span>bootstrap-token<span class=o>]</span> configured RBAC rules to allow certificate rotation <span class=k>for</span> all node client certificates in the cluster
</span></span><span class=line><span class=cl><span class=o>[</span>addons<span class=o>]</span>: Migrating CoreDNS Corefile
</span></span><span class=line><span class=cl><span class=o>[</span>addons<span class=o>]</span> Applied essential addon: CoreDNS
</span></span><span class=line><span class=cl><span class=o>[</span>addons<span class=o>]</span> Applied essential addon: kube-proxy
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/successful<span class=o>]</span> SUCCESS! Your cluster was upgraded to <span class=s2>&#34;v1.17.5&#34;</span>. Enjoy!
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade/kubelet<span class=o>]</span> Now that your control plane is upgraded, please proceed with upgrading your kubelets <span class=k>if</span> you haven<span class=err>&#39;</span>t already <span class=k>done</span> so.
</span></span></code></pre></div><h3 id=解除-master-节点污点>解除 master 节点污点<a hidden class=anchor aria-hidden=true href=#解除-master-节点污点>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ kubectl uncordon k8s-master
</span></span><span class=line><span class=cl>node/k8s-master uncordoned
</span></span></code></pre></div><h3 id=升级其他-master-节点>升级其他 master 节点<a hidden class=anchor aria-hidden=true href=#升级其他-master-节点>#</a></h3><p>如果是高可用版本还需要升级其他的 master 节点</p><blockquote><p>本次集群只有单个 master,以下操作纯做记录</p></blockquote><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>yum install -y kubeadm-1.17.5-0 --disableexcludes<span class=o>=</span>kubernetes
</span></span><span class=line><span class=cl>kubeadm upgrade node
</span></span><span class=line><span class=cl>kubeadm upgrade apply
</span></span></code></pre></div><h3 id=升级所有-master-节点的-kubectl-与-kubelet>升级所有 master 节点的 kubectl 与 kubelet<a hidden class=anchor aria-hidden=true href=#升级所有-master-节点的-kubectl-与-kubelet>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>yum install -y kubectl-1.17.5-0 kubelet-1.17.5-0 --disableexcludes<span class=o>=</span>kubernetes
</span></span></code></pre></div><h3 id=重启-master-节点-kubelet-服务>重启 master 节点 kubelet 服务<a hidden class=anchor aria-hidden=true href=#重启-master-节点-kubelet-服务>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ systemctl daemon-reload
</span></span><span class=line><span class=cl>$ systemctl restart kubelet
</span></span><span class=line><span class=cl>$ systemctl status kubelet
</span></span><span class=line><span class=cl>● kubelet.service - kubelet: The Kubernetes Node Agent
</span></span><span class=line><span class=cl>   Loaded: loaded <span class=o>(</span>/usr/lib/systemd/system/kubelet.service<span class=p>;</span> enabled<span class=p>;</span> vendor preset: disabled<span class=o>)</span>
</span></span><span class=line><span class=cl>  Drop-In: /usr/lib/systemd/system/kubelet.service.d
</span></span><span class=line><span class=cl>           └─10-kubeadm.conf
</span></span><span class=line><span class=cl>   Active: active <span class=o>(</span>running<span class=o>)</span> since Wed 2020-05-20 20:11:47 CST<span class=p>;</span> 12s ago
</span></span><span class=line><span class=cl>     Docs: https://kubernetes.io/docs/
</span></span><span class=line><span class=cl> Main PID: <span class=m>2904</span> <span class=o>(</span>kubelet<span class=o>)</span>
</span></span><span class=line><span class=cl>    Tasks: <span class=m>15</span>
</span></span><span class=line><span class=cl>   Memory: 25.9M
</span></span><span class=line><span class=cl>   CGroup: /system.slice/kubelet.service
</span></span><span class=line><span class=cl>           └─2904 /usr/bin/kubelet --bootstrap-kubeconfig<span class=o>=</span>/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig<span class=o>=</span>/etc/kubernetes/kubelet.conf --config<span class=o>=</span>/var/lib/kubelet/config.yaml --cgroup-driver<span class=o>=</span>systemd --network-plugin<span class=o>=</span>cni --pod-infra...
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>May <span class=m>20</span> 20:11:47 k8s-master kubelet<span class=o>[</span>2904<span class=o>]</span>: I0520 20:11:47.297946    <span class=m>2904</span> docker_service.go:260<span class=o>]</span> Docker Info: <span class=p>&amp;</span><span class=o>{</span>ID:BI53:OQC7:QV6C:SP3N:FBTT:N5ST:MK3I:CGLJ:X2JX:CJOH:CBEE:4GG2 Containers:30 ContainersRunning:22 Containe...rts d_type true<span class=o>]</span>
</span></span><span class=line><span class=cl>May <span class=m>20</span> 20:11:47 k8s-master kubelet<span class=o>[</span>2904<span class=o>]</span>: I0520 20:11:47.298023    <span class=m>2904</span> docker_service.go:273<span class=o>]</span> Setting cgroupDriver to systemd
</span></span><span class=line><span class=cl>May <span class=m>20</span> 20:11:47 k8s-master kubelet<span class=o>[</span>2904<span class=o>]</span>: I0520 20:11:47.305372    <span class=m>2904</span> remote_runtime.go:59<span class=o>]</span> parsed scheme: <span class=s2>&#34;&#34;</span>
</span></span><span class=line><span class=cl>May <span class=m>20</span> 20:11:47 k8s-master kubelet<span class=o>[</span>2904<span class=o>]</span>: I0520 20:11:47.305387    <span class=m>2904</span> remote_runtime.go:59<span class=o>]</span> scheme <span class=s2>&#34;&#34;</span> not registered, fallback to default scheme
</span></span><span class=line><span class=cl>May <span class=m>20</span> 20:11:47 k8s-master kubelet<span class=o>[</span>2904<span class=o>]</span>: I0520 20:11:47.305408    <span class=m>2904</span> passthrough.go:48<span class=o>]</span> ccResolverWrapper: sending update to cc: <span class=o>{[{</span>/var/run/dockershim.sock <span class=m>0</span>  &lt;nil&gt;<span class=o>}]</span> &lt;nil&gt;<span class=o>}</span>
</span></span><span class=line><span class=cl>May <span class=m>20</span> 20:11:47 k8s-master kubelet<span class=o>[</span>2904<span class=o>]</span>: I0520 20:11:47.305415    <span class=m>2904</span> clientconn.go:577<span class=o>]</span> ClientConn switching balancer to <span class=s2>&#34;pick_first&#34;</span>
</span></span><span class=line><span class=cl>May <span class=m>20</span> 20:11:47 k8s-master kubelet<span class=o>[</span>2904<span class=o>]</span>: I0520 20:11:47.305433    <span class=m>2904</span> remote_image.go:50<span class=o>]</span> parsed scheme: <span class=s2>&#34;&#34;</span>
</span></span><span class=line><span class=cl>May <span class=m>20</span> 20:11:47 k8s-master kubelet<span class=o>[</span>2904<span class=o>]</span>: I0520 20:11:47.305438    <span class=m>2904</span> remote_image.go:50<span class=o>]</span> scheme <span class=s2>&#34;&#34;</span> not registered, fallback to default scheme
</span></span><span class=line><span class=cl>May <span class=m>20</span> 20:11:47 k8s-master kubelet<span class=o>[</span>2904<span class=o>]</span>: I0520 20:11:47.305446    <span class=m>2904</span> passthrough.go:48<span class=o>]</span> ccResolverWrapper: sending update to cc: <span class=o>{[{</span>/var/run/dockershim.sock <span class=m>0</span>  &lt;nil&gt;<span class=o>}]</span> &lt;nil&gt;<span class=o>}</span>
</span></span><span class=line><span class=cl>May <span class=m>20</span> 20:11:47 k8s-master kubelet<span class=o>[</span>2904<span class=o>]</span>: I0520 20:11:47.305450    <span class=m>2904</span> clientconn.go:577<span class=o>]</span> ClientConn switching balancer to <span class=s2>&#34;pick_first&#34;</span>
</span></span><span class=line><span class=cl>Hint: Some lines were ellipsized, use -l to show in full.
</span></span></code></pre></div><h3 id=确认-master-版本>确认 master 版本<a hidden class=anchor aria-hidden=true href=#确认-master-版本>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ kubectl get node k8s-master
</span></span><span class=line><span class=cl>NAME         STATUS   ROLES    AGE    VERSION
</span></span><span class=line><span class=cl>k8s-master   Ready    master   180d   v1.17.5
</span></span></code></pre></div><h2 id=升级-worker-节点>升级 worker 节点<a hidden class=anchor aria-hidden=true href=#升级-worker-节点>#</a></h2><blockquote><p>非特别说明，都是在 worker 节点上操作</p></blockquote><p>需要在所有 woker 节点上进行操作, 此处以 k8s-node01 节点为例</p><h3 id=更新-kubeadm>更新 kubeadm<a hidden class=anchor aria-hidden=true href=#更新-kubeadm>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>yum install -y kubeadm-1.17.5-0 --disableexcludes<span class=o>=</span>kubernetes
</span></span></code></pre></div><h3 id=给-worker-节点打上污点并驱逐所有-pod>给 worker 节点打上污点并驱逐所有 pod<a hidden class=anchor aria-hidden=true href=#给-worker-节点打上污点并驱逐所有-pod>#</a></h3><blockquote><p>在 Master 节点上执行</p></blockquote><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ kubectl drain k8s-node01 --ignore-daemonsets --delete-local-data
</span></span><span class=line><span class=cl>node/k8s-node01 cordoned
</span></span><span class=line><span class=cl>evicting pod <span class=s2>&#34;tekton-pipelines-webhook-694dc67fbb-kjbqv&#34;</span>
</span></span><span class=line><span class=cl>...
</span></span><span class=line><span class=cl>pod/tekton-pipelines-webhook-694dc67fbb-kjbqv evicted
</span></span><span class=line><span class=cl>...
</span></span><span class=line><span class=cl>node/k8s-node01 evicted
</span></span></code></pre></div><h3 id=升级-woker-节点-kubelet-配置>升级 woker 节点 kubelet 配置<a hidden class=anchor aria-hidden=true href=#升级-woker-节点-kubelet-配置>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ kubeadm upgrade node
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade<span class=o>]</span> Reading configuration from the cluster...
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade<span class=o>]</span> FYI: You can look at this config file with <span class=s1>&#39;kubectl -n kube-system get cm kubeadm-config -oyaml&#39;</span>
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade<span class=o>]</span> Skipping phase. Not a control plane node.
</span></span><span class=line><span class=cl><span class=o>[</span>kubelet-start<span class=o>]</span> Downloading configuration <span class=k>for</span> the kubelet from the <span class=s2>&#34;kubelet-config-1.17&#34;</span> ConfigMap in the kube-system namespace
</span></span><span class=line><span class=cl><span class=o>[</span>kubelet-start<span class=o>]</span> Writing kubelet configuration to file <span class=s2>&#34;/var/lib/kubelet/config.yaml&#34;</span>
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade<span class=o>]</span> The configuration <span class=k>for</span> this node was successfully updated!
</span></span><span class=line><span class=cl><span class=o>[</span>upgrade<span class=o>]</span> Now you should go ahead and upgrade the kubelet package using your package manager.
</span></span></code></pre></div><h3 id=升级-woker-节点-kubelet-与-kubectl-版本>升级 woker 节点 kubelet 与 kubectl 版本<a hidden class=anchor aria-hidden=true href=#升级-woker-节点-kubelet-与-kubectl-版本>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>yum install -y kubectl-1.17.5-0 kubelet-1.17.5-0 --disableexcludes<span class=o>=</span>kubernetes
</span></span></code></pre></div><h3 id=重启-kubelet-服务>重启 kubelet 服务<a hidden class=anchor aria-hidden=true href=#重启-kubelet-服务>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ systemctl daemon-reload
</span></span><span class=line><span class=cl>$ systemctl restart kubelet
</span></span><span class=line><span class=cl>$ systemctl status kubelet
</span></span><span class=line><span class=cl>● kubelet.service - kubelet: The Kubernetes Node Agent
</span></span><span class=line><span class=cl>   Loaded: loaded <span class=o>(</span>/usr/lib/systemd/system/kubelet.service<span class=p>;</span> enabled<span class=p>;</span> vendor preset: disabled<span class=o>)</span>
</span></span><span class=line><span class=cl>  Drop-In: /usr/lib/systemd/system/kubelet.service.d
</span></span><span class=line><span class=cl>           └─10-kubeadm.conf
</span></span><span class=line><span class=cl>   Active: active <span class=o>(</span>running<span class=o>)</span> since Wed 2020-05-20 20:27:07 CST<span class=p>;</span> 21s ago
</span></span><span class=line><span class=cl>     Docs: https://kubernetes.io/docs/
</span></span><span class=line><span class=cl> Main PID: <span class=m>4202</span> <span class=o>(</span>kubelet<span class=o>)</span>
</span></span><span class=line><span class=cl>    Tasks: <span class=m>18</span>
</span></span><span class=line><span class=cl>   Memory: 38.0M
</span></span><span class=line><span class=cl>   CGroup: /system.slice/kubelet.service
</span></span><span class=line><span class=cl>           └─4202 /usr/bin/kubelet --bootstrap-kubeconfig<span class=o>=</span>/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig<span class=o>=</span>/et...
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>May <span class=m>20</span> 20:27:28 k8s-node01 kubelet<span class=o>[</span>4202<span class=o>]</span>: I0520 20:27:28.637150    <span class=m>4202</span> kuberuntime_manager.go:981<span class=o>]</span> updating....0/24
</span></span><span class=line><span class=cl>May <span class=m>20</span> 20:27:28 k8s-node01 kubelet<span class=o>[</span>4202<span class=o>]</span>: I0520 20:27:28.637283    <span class=m>4202</span> docker_service.go:355<span class=o>]</span> docker cri re...4,<span class=o>}</span>,<span class=o>}</span>
</span></span><span class=line><span class=cl>May <span class=m>20</span> 20:27:28 k8s-node01 kubelet<span class=o>[</span>4202<span class=o>]</span>: I0520 20:27:28.637397    <span class=m>4202</span> kubelet_network.go:77<span class=o>]</span> Setting Pod C....0/24
</span></span><span class=line><span class=cl>May <span class=m>20</span> 20:27:28 k8s-node01 kubelet<span class=o>[</span>4202<span class=o>]</span>: I0520 20:27:28.638934    <span class=m>4202</span> kubelet_node_status.go:70<span class=o>]</span> Attemptin...ode01
</span></span><span class=line><span class=cl>May <span class=m>20</span> 20:27:28 k8s-node01 kubelet<span class=o>[</span>4202<span class=o>]</span>: I0520 20:27:28.653131    <span class=m>4202</span> kubelet_node_status.go:112<span class=o>]</span> Node k8s...tered
</span></span><span class=line><span class=cl>May <span class=m>20</span> 20:27:28 k8s-node01 kubelet<span class=o>[</span>4202<span class=o>]</span>: I0520 20:27:28.653222    <span class=m>4202</span> kubelet_node_status.go:73<span class=o>]</span> Successfu...ode01
</span></span><span class=line><span class=cl>May <span class=m>20</span> 20:27:28 k8s-node01 kubelet<span class=o>[</span>4202<span class=o>]</span>: E0520 20:27:28.657774    <span class=m>4202</span> kubelet.go:1844<span class=o>]</span> skipping pod synchr...d yet
</span></span><span class=line><span class=cl>May <span class=m>20</span> 20:27:28 k8s-node01 kubelet<span class=o>[</span>4202<span class=o>]</span>: I0520 20:27:28.659901    <span class=m>4202</span> setters.go:537<span class=o>]</span> Node became not ready: <span class=o>{</span>T...
</span></span><span class=line><span class=cl>May <span class=m>20</span> 20:27:28 k8s-node01 kubelet<span class=o>[</span>4202<span class=o>]</span>: W0520 20:27:28.842260    <span class=m>4202</span> docker_sandbox.go:394<span class=o>]</span> failed to <span class=nb>read</span> pod...
</span></span><span class=line><span class=cl>May <span class=m>20</span> 20:27:28 k8s-node01 kubelet<span class=o>[</span>4202<span class=o>]</span>: E0520 20:27:28.857932    <span class=m>4202</span> kubelet.go:1844<span class=o>]</span> skipping pod synchr...d yet
</span></span><span class=line><span class=cl>Hint: Some lines were ellipsized, use -l to show in full.
</span></span></code></pre></div><h3 id=解除-worker-节点污点>解除 worker 节点污点<a hidden class=anchor aria-hidden=true href=#解除-worker-节点污点>#</a></h3><blockquote><p>在 Master 节点上操作</p></blockquote><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ kubectl uncordon k8s-node01
</span></span><span class=line><span class=cl>node/k8s-node01 uncordoned
</span></span></code></pre></div><h3 id=验证-worker-节点版本>验证 worker 节点版本<a hidden class=anchor aria-hidden=true href=#验证-worker-节点版本>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ kubectl get nodes
</span></span><span class=line><span class=cl>NAME         STATUS   ROLES    AGE    VERSION
</span></span><span class=line><span class=cl>k8s-master   Ready    master   180d   v1.17.2
</span></span><span class=line><span class=cl>k8s-node01   Ready    &lt;none&gt;   180d   v1.17.5
</span></span><span class=line><span class=cl>k8s-node02   Ready    &lt;none&gt;   180d   v1.17.5
</span></span><span class=line><span class=cl>k8s-node03   Ready    &lt;none&gt;   180d   v1.17.5
</span></span><span class=line><span class=cl>k8s-node04   Ready    &lt;none&gt;   180d   v1.17.5
</span></span><span class=line><span class=cl>k8s-node05   Ready    &lt;none&gt;   180d   v1.17.5
</span></span><span class=line><span class=cl>k8s-node06   Ready    &lt;none&gt;   180d   v1.17.5
</span></span></code></pre></div><p>如上，再以相同的方法更新到 1.18.x 版本即可</p><h2 id=验证集群状态>验证集群状态<a hidden class=anchor aria-hidden=true href=#验证集群状态>#</a></h2><h3 id=检查系统组件>检查系统组件<a hidden class=anchor aria-hidden=true href=#检查系统组件>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ kubectl get pods -n kube-system
</span></span><span class=line><span class=cl>NAME                                       READY   STATUS    RESTARTS   AGE
</span></span><span class=line><span class=cl>calico-kube-controllers-7d4d547dd6-59xqn   1/1     Running   <span class=m>0</span>          17m
</span></span><span class=line><span class=cl>calico-node-8d8bm                          1/1     Running   <span class=m>0</span>          19h
</span></span><span class=line><span class=cl>calico-node-hlv2h                          1/1     Running   <span class=m>0</span>          19h
</span></span><span class=line><span class=cl>calico-node-nbt4b                          1/1     Running   <span class=m>0</span>          19h
</span></span><span class=line><span class=cl>calico-node-p4s45                          1/1     Running   <span class=m>4</span>          19h
</span></span><span class=line><span class=cl>calico-node-srb88                          1/1     Running   <span class=m>0</span>          19h
</span></span><span class=line><span class=cl>calico-node-vfpfv                          1/1     Running   <span class=m>0</span>          19h
</span></span><span class=line><span class=cl>coredns-546565776c-54gxr                   1/1     Running   <span class=m>0</span>          12m
</span></span><span class=line><span class=cl>coredns-546565776c-zqhcb                   1/1     Running   <span class=m>0</span>          20m
</span></span><span class=line><span class=cl>etcd-k8s-master                            1/1     Running   <span class=m>0</span>          20m
</span></span><span class=line><span class=cl>istio-cni-node-42ngc                       2/2     Running   <span class=m>0</span>          10h
</span></span><span class=line><span class=cl>istio-cni-node-6qngn                       2/2     Running   <span class=m>0</span>          10h
</span></span><span class=line><span class=cl>istio-cni-node-8pzlj                       2/2     Running   <span class=m>0</span>          10h
</span></span><span class=line><span class=cl>istio-cni-node-9p47g                       2/2     Running   <span class=m>0</span>          10h
</span></span><span class=line><span class=cl>istio-cni-node-hqp75                       2/2     Running   <span class=m>0</span>          10h
</span></span><span class=line><span class=cl>istio-cni-node-vmnbp                       2/2     Running   <span class=m>10</span>         10h
</span></span><span class=line><span class=cl>kube-apiserver-k8s-master                  1/1     Running   <span class=m>0</span>          20m
</span></span><span class=line><span class=cl>kube-controller-manager-k8s-master         1/1     Running   <span class=m>0</span>          20m
</span></span><span class=line><span class=cl>kube-proxy-86845                           1/1     Running   <span class=m>0</span>          19m
</span></span><span class=line><span class=cl>kube-proxy-dnccs                           1/1     Running   <span class=m>0</span>          20m
</span></span><span class=line><span class=cl>kube-proxy-grn6t                           1/1     Running   <span class=m>0</span>          19m
</span></span><span class=line><span class=cl>kube-proxy-ltwdr                           1/1     Running   <span class=m>0</span>          19m
</span></span><span class=line><span class=cl>kube-proxy-p5mn8                           1/1     Running   <span class=m>0</span>          19m
</span></span><span class=line><span class=cl>kube-proxy-qrnb7                           1/1     Running   <span class=m>0</span>          19m
</span></span><span class=line><span class=cl>kube-scheduler-k8s-master                  1/1     Running   <span class=m>0</span>          20m
</span></span><span class=line><span class=cl>metrics-server-747dfbd64b-5nnlz            1/1     Running   <span class=m>0</span>          23s
</span></span></code></pre></div><h3 id=测试-dns-功能>测试 DNS 功能<a hidden class=anchor aria-hidden=true href=#测试-dns-功能>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ dig www.baidu.com @10.96.0.10 +short
</span></span><span class=line><span class=cl>www.a.shifen.com.
</span></span><span class=line><span class=cl>112.80.248.75
</span></span><span class=line><span class=cl>112.80.248.76
</span></span><span class=line><span class=cl>$ dig kubernetes.default.svc.cluster.local @10.96.0.10 +short
</span></span><span class=line><span class=cl>10.96.0.1
</span></span><span class=line><span class=cl>$ kubectl run -n kube-ops -i --rm --restart<span class=o>=</span>Never dummy --image<span class=o>=</span>busybox:1.28 -- nslookup kubernetes.default
</span></span><span class=line><span class=cl>Server:    10.96.0.10
</span></span><span class=line><span class=cl>Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local
</span></span><span class=line><span class=cl>Name:      kubernetes.default
</span></span><span class=line><span class=cl>Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local
</span></span><span class=line><span class=cl>pod <span class=s2>&#34;dummy&#34;</span> deleted
</span></span></code></pre></div><h3 id=测试访问集群中的服务>测试访问集群中的服务<a hidden class=anchor aria-hidden=true href=#测试访问集群中的服务>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ kubectl get service -n default
</span></span><span class=line><span class=cl>NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT<span class=o>(</span>S<span class=o>)</span>   AGE
</span></span><span class=line><span class=cl>whoami       ClusterIP   10.96.179.57   &lt;none&gt;        80/TCP    21d
</span></span><span class=line><span class=cl>$ curl http://10.96.179.57
</span></span><span class=line><span class=cl>Hostname: whoami-5b4bb9c787-xz87j
</span></span><span class=line><span class=cl>IP: 127.0.0.1
</span></span><span class=line><span class=cl>IP: 172.16.217.76
</span></span><span class=line><span class=cl>RemoteAddr: 127.0.0.1:33338
</span></span><span class=line><span class=cl>GET / HTTP/1.1
</span></span><span class=line><span class=cl>Host: 10.96.179.57
</span></span><span class=line><span class=cl>User-Agent: curl/7.29.0
</span></span><span class=line><span class=cl>Accept: */*
</span></span><span class=line><span class=cl>$ kubectl run curl -n kube-ops -it --image curlimages/curl --restart Never --rm -- curl whoami.default
</span></span><span class=line><span class=cl>Hostname: whoami-5b4bb9c787-xz87j
</span></span><span class=line><span class=cl>IP: 127.0.0.1
</span></span><span class=line><span class=cl>IP: 172.16.217.76
</span></span><span class=line><span class=cl>RemoteAddr: 127.0.0.1:35506
</span></span><span class=line><span class=cl>GET / HTTP/1.1
</span></span><span class=line><span class=cl>Host: whoami.default
</span></span><span class=line><span class=cl>User-Agent: curl/7.70.0-DEV
</span></span><span class=line><span class=cl>Accept: */*
</span></span><span class=line><span class=cl>pod <span class=s2>&#34;curl&#34;</span> deleted
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://linuxyunwei.com/tags/kubeadm/>Kubeadm</a></li><li><a href=https://linuxyunwei.com/tags/kubernetes/>Kubernetes</a></li></ul><nav class=paginav><a class=prev href=https://linuxyunwei.com/2025/01/14/%E5%9C%A8%E9%98%BF%E9%87%8C%E4%BA%91ecs%E4%B8%AD%E9%83%A8%E7%BD%B2k3s-higress/><span class=title>«</span><br><span>在阿里云ECS中部署k3s+Higress</span>
</a><a class=next href=https://linuxyunwei.com/2019/11/18/kubeadm-%E5%AE%89%E8%A3%85%E5%8D%95master%E9%9B%86%E7%BE%A4/><span class=title>»</span><br><span>kubeadm 安装单master集群</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 使用Kubeadm更新集群 on x" href="https://x.com/intent/tweet/?text=%e4%bd%bf%e7%94%a8Kubeadm%e6%9b%b4%e6%96%b0%e9%9b%86%e7%be%a4&amp;url=https%3a%2f%2flinuxyunwei.com%2f2020%2f05%2f20%2f%25E4%25BD%25BF%25E7%2594%25A8kubeadm%25E6%259B%25B4%25E6%2596%25B0%25E9%259B%2586%25E7%25BE%25A4%2f&amp;hashtags=kubeadm%2ckubernetes"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 使用Kubeadm更新集群 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flinuxyunwei.com%2f2020%2f05%2f20%2f%25E4%25BD%25BF%25E7%2594%25A8kubeadm%25E6%259B%25B4%25E6%2596%25B0%25E9%259B%2586%25E7%25BE%25A4%2f&amp;title=%e4%bd%bf%e7%94%a8Kubeadm%e6%9b%b4%e6%96%b0%e9%9b%86%e7%be%a4&amp;summary=%e4%bd%bf%e7%94%a8Kubeadm%e6%9b%b4%e6%96%b0%e9%9b%86%e7%be%a4&amp;source=https%3a%2f%2flinuxyunwei.com%2f2020%2f05%2f20%2f%25E4%25BD%25BF%25E7%2594%25A8kubeadm%25E6%259B%25B4%25E6%2596%25B0%25E9%259B%2586%25E7%25BE%25A4%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 使用Kubeadm更新集群 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2flinuxyunwei.com%2f2020%2f05%2f20%2f%25E4%25BD%25BF%25E7%2594%25A8kubeadm%25E6%259B%25B4%25E6%2596%25B0%25E9%259B%2586%25E7%25BE%25A4%2f&title=%e4%bd%bf%e7%94%a8Kubeadm%e6%9b%b4%e6%96%b0%e9%9b%86%e7%be%a4"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 使用Kubeadm更新集群 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flinuxyunwei.com%2f2020%2f05%2f20%2f%25E4%25BD%25BF%25E7%2594%25A8kubeadm%25E6%259B%25B4%25E6%2596%25B0%25E9%259B%2586%25E7%25BE%25A4%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 使用Kubeadm更新集群 on whatsapp" href="https://api.whatsapp.com/send?text=%e4%bd%bf%e7%94%a8Kubeadm%e6%9b%b4%e6%96%b0%e9%9b%86%e7%be%a4%20-%20https%3a%2f%2flinuxyunwei.com%2f2020%2f05%2f20%2f%25E4%25BD%25BF%25E7%2594%25A8kubeadm%25E6%259B%25B4%25E6%2596%25B0%25E9%259B%2586%25E7%25BE%25A4%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 使用Kubeadm更新集群 on telegram" href="https://telegram.me/share/url?text=%e4%bd%bf%e7%94%a8Kubeadm%e6%9b%b4%e6%96%b0%e9%9b%86%e7%be%a4&amp;url=https%3a%2f%2flinuxyunwei.com%2f2020%2f05%2f20%2f%25E4%25BD%25BF%25E7%2594%25A8kubeadm%25E6%259B%25B4%25E6%2596%25B0%25E9%259B%2586%25E7%25BE%25A4%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 使用Kubeadm更新集群 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%e4%bd%bf%e7%94%a8Kubeadm%e6%9b%b4%e6%96%b0%e9%9b%86%e7%be%a4&u=https%3a%2f%2flinuxyunwei.com%2f2020%2f05%2f20%2f%25E4%25BD%25BF%25E7%2594%25A8kubeadm%25E6%259B%25B4%25E6%2596%25B0%25E9%259B%2586%25E7%25BE%25A4%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>戴先森 · <a rel="license noopener" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a> <a href=http://www.miitbeian.gov.cn/ target=_blank>沪ICP备13017200号-1</a> <a target=_blank rel=noreferrer href="https://beian.mps.gov.cn/#/query/webSearch?code=31011702890206">沪公网安备31011702890206号</a><br></span>·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>